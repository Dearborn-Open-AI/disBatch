<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Distributed processing of a batch of tasks. &mdash; HumanBase Pipeline 0.1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="#" class="icon icon-home"> HumanBase Pipeline
          </a>
              <div class="version">
                0.1
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Distributed processing of a batch of tasks.</a><ul>
<li><a class="reference internal" href="#invocation">## Invocation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#contents">Contents:</a></li>
<li><a class="reference internal" href="#feedback">Feedback</a></li>
</ul>
</div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">HumanBase Pipeline</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="#" class="icon icon-home"></a> &raquo;</li>
      <li>Distributed processing of a batch of tasks.</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="distributed-processing-of-a-batch-of-tasks">
<h1>Distributed processing of a batch of tasks.<a class="headerlink" href="#distributed-processing-of-a-batch-of-tasks" title="Permalink to this headline"></a></h1>
<p>## Overview</p>
<p>One common usage pattern for distributed computing involves processing a
long list of commands (aka <em>tasks</em>):</p>
<blockquote>
<div><p>( cd /path/to/workdir ; source SetupEnv ; myprog -a 0 -b 0 -c 0 ) &amp;&gt; task_0_0_0.log
( cd /path/to/workdir ; source SetupEnv ; myprog -a 0 -b 0 -c 1 ) &amp;&gt; task_0_0_1.log
…
( cd /path/to/workdir ; source SetupEnv ; myprog -a 9 -b 9 -c 8 ) &amp;&gt; task_9_9_8.log
( cd /path/to/workdir ; source SetupEnv ; myprog -a 9 -b 9 -c 9 ) &amp;&gt; task_9_9_9.log</p>
</div></blockquote>
<p>One could do this by submitting 1,000 separate jobs to a cluster computer, but that may
present problems for the queuing system and can behave badly if the
system is configured to handle jobs in a simple first come, first serve
fashion.</p>
<p>Another alternative is to use a resource management system’s native
support for job arrays, but this often requires massaging the commands
to reflect syntax specific to a particular system’s implementation of
job arrays.</p>
<p>And what if you don’t have a cluster available, but do have a collection of networked computers? Or you just want to make use of multiple cores on your own computer?</p>
<p>In any event, when processing such a list of tasks, it is helpful to
acquire metadata about the execution of each task: where it ran, how
long it took, its exit return code, etc.</p>
<p><strong>disBatch</strong> has been designed to support this usage in a simple and
portable way, as well as to provide the sort of metadata that can be
helpful for debugging and reissuing failed tasks.</p>
<p>It can take as input a file, each of whose lines is a task in the form of a
command sequence. For example, the file could consists of the 1000 commands listed above. It launches the tasks one
after the other until all specified execution resources are in use. Then as one
executing task exits, the next task in the file is launched. This repeats until all
the lines in the file have been processed.</p>
<p>Each task is run in a new shell. If you want to manipulate the execution environment of a task, add the appropriate operations to the command sequence&amp;mdash;`source SetupEnv` in the above is one example. For another, if you just need to set an environment variable, each task line would look something like:</p>
<blockquote>
<div><p>export PYTHONPATH=/d0/d1/d2:$PYTHONPATH ; rest ; of ; command ; sequence</p>
</div></blockquote>
<p>Or, for more complex set ups, command sequences and input/output redirection requirements, you could place everything in a small shell script with appropriate arguments for the parts that vary from task to task, say RunMyprog.sh:</p>
<blockquote>
<div><p>#!/bin/bash</p>
<p>id=$1
shift
cd /path/to/workdir
module purge
module load gcc python3</p>
<p>export PYTHONPATH=/d0/d1/d2:$PYTHONPATH
myProg “$&#64;” &gt; results/${id}.out 2&gt; logs/${id}.log</p>
</div></blockquote>
<p>The task file would then contain:</p>
<blockquote>
<div><p>./RunMyprog.sh 0_0_0 -a 0 -b 0 -c 0
./RunMyprog.sh 0_0_1 -a 0 -b 0 -c 1
…
./RunMyprog.sh 9_9_8 -a 9 -b 9 -c 8
./RunMyprog.sh 9_9_9 -a 9 -b 9 -c 9</p>
</div></blockquote>
<p>See #DISBATCH directives below for ways to simplify task lines.</p>
<p>Once you have created the task file, running disBatch is straightforward. For example, working with a cluster managed by SLURM,
all that needs to be done is to submit a job like the following:</p>
<blockquote>
<div><p>sbatch -n 20 –ntasks-per-node 5 disBatch TaskFileName</p>
</div></blockquote>
<p>This particular invocation will allocate sufficient resources to process
20 tasks at a time, with no more than five running concurrently on any
given node. disBatch will use environment variables initialized by SLURM to determine the execution resources to use for the run.
This invocation assumes an appropriately installed disBatch is in your PATH, see below for installation notes.</p>
<p>Various log files will be created as the run unfolds:</p>
<ul class="simple">
<li><p><cite>TaskFileName_134504_status.txt</cite> (assuming the SLURM Job ID is <cite>134504</cite>): status of every task (details below)</p></li>
<li><p><cite>disBatch_134504_driver.txt</cite> (can be changed with <cite>-l</cite>), <cite>TaskFileName_134504_worker032_engine.txt</cite>:
The disBatch log file contains details mostly of interest in case of a
problem with disBatch itself. It can generally be ignored by end
users (but keep it around in the event that something did go
wrong&amp;mdash;it will aid debugging). The <a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">*</span></a>_engine.txt’’ files contain similar information for each node acting as an execution resource</p></li>
<li><p><cite>disBatch_134504_kvsinfo.txt</cite>: TCP address of invoked KVS server if any (for additional advanced status monitoring)</p></li>
</ul>
<p>### Status file</p>
<p>The <cite>_status.txt</cite> file contains tab-delimited lines of the form:</p>
<blockquote>
<div><p>314 315     -1      worker032       8016    0       10.0486528873   1458660919.78   1458660929.83   0       “”      0       “”      ‘cd /path/to/workdir ; myprog -a 3 -b 1 -c 4 &gt; task_3_1_4.log 2&gt;&amp;1’</p>
</div></blockquote>
<p>These fields are:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Flags: The first field, blank in this case, may contain <cite>E</cite>, <cite>O</cite>, <cite>R</cite>, <cite>B</cite>, or <cite>S</cite> flags.
Each program/task should be invoked in such a way that standard error
and standard output end up in appropriate files. If that’s not the case
<cite>E</cite> or <cite>O</cite> flags will be raised. <cite>R</cite> indicates that the task
returned a non-zero exit code. <cite>B</cite> indicates a barrier (see below). <cite>S</cite> indicates the job was skipped (this may happen during “resume” runs).</p></li>
</ol>
<p>1. Task ID: The <cite>314</cite> is the 0-based index of the task (starting from the beginning of the task file, incremented for each task, including repeats).
1. Line number: The <cite>315</cite> is the 1-based line from the task file. Blank lines, comments, directives and repeats may cause this to drift considerably from the value of Task ID.
1. Repeat index: The <cite>-1</cite> is the repeat index (as in this example, <cite>-1</cite> indicates this task was not part of a repeat directive).
1. Node: <cite>worker032</cite> identifies the node on which the task ran.
1. PID: <cite>8016</cite> is the PID of the bash shell used to run the task.
1. Exit code: <cite>0</cite> is the exit code returned.
1. Elapsed time: <cite>10.0486528873</cite> (seconds),
1. Start time:<cite>1458660919.78</cite> (epoch based),
1. Finish time: <cite>1458660929.83</cite> (epoch based).
1. Bytes of <em>leaked</em> output (not redirected to a file),
1. Output snippet (up to 80 bytes consisting of the prefix and suffix of the output),
1. Bytes of leaked error output,
1. Error snippet,
1. Command: <cite>cd …</cite> is the text of the task (repeated from the task file, but see below).</p>
</div></blockquote>
<p>## Installation</p>
<p><strong>Users of Flatiron resources: disBatch is available via the module system. You do not need to clone this repo to use it.</strong></p>
<p><cite>disBatch.py</cite> requires the <cite>kvsstcp</cite> package, which should be installed in python’s path, or placed in this directory.
You can simply clone this git repository with <cite>–recursive</cite> (or run <cite>git submodule update –init</cite> if you’ve already cloned it).</p>
<p>Depending on your execution environment, the ability of disBatch to determine the location of itself and kvsstcp may be disrupted. To avoid such problems, set the environment variable <cite>DISBATCH_ROOT</cite> to the path of the directory containing <cite>disBatch.py</cite>.</p>
<p>disBatch is designed to support a variety of execution environments, from your own desktop, to a local collection of workstations, to large clusters managed by job schedulers.
It currently supports SLURM and can be executed from <cite>sbatch</cite>, but it is architected to make it simple to add support for other resource managers.</p>
<p>You can also run directly on one or more machines by setting an environment variable:</p>
<blockquote>
<div><p>DISBATCH_SSH_NODELIST=localhost:7,otherhost:3</p>
</div></blockquote>
<p>or specifying an invocation argument:</p>
<blockquote>
<div><p>-s localhost:7,otherhost:3</p>
</div></blockquote>
<p>This allows execution directly on your <cite>localhost</cite> and via ssh for remote hosts without the need for a resource management system.
In this example, disBatch is told it can use seven CPUs on your local host and three on <cite>otherhost</cite>. Assuming the default mapping of one task to one CPU applies in this example, seven tasks could be in progress at any given time on <cite>localhost</cite>, and three on <cite>otherhost</cite>. Note that <cite>localhost</cite> is an actual name you can use to refer to the machine on which you are currently working. <cite>otherhost</cite> is fictious.
Hosts used via ssh must be set up to allow ssh to work without a password and must share the working directory for the disBatch run.</p>
<section id="invocation">
<h2>## Invocation<a class="headerlink" href="#invocation" title="Permalink to this headline"></a></h2>
<dl class="simple">
<dt>usage: disBatch [-h] [-p PATH] [–logfile FILE] [–mailFreq N] [–mailTo ADDR] [-S] [-r STATUSFILE]</dt><dd><p>[-R] [–force-resume] [-e] [-w] [–kvsserver [HOST:PORT]] [–taskcommand COMMAND]
[–taskserver [HOST:PORT]] [-C TASK_LIMIT] [-c N] [-g] [-k COMMAND] [-K]
[-l COMMAND] [-s HOST:COUNT] [-t N]
[taskfile]</p>
</dd>
</dl>
<p>Use batch resources to process a file of tasks, one task per line.</p>
<dl>
<dt>positional arguments:</dt><dd><p>taskfile              File with tasks, one task per line (“-” for stdin)</p>
</dd>
<dt>optional arguments:</dt><dd><dl class="option-list">
<dt><kbd><span class="option">-h</span>, <span class="option">--help</span></kbd></dt>
<dd><p>show this help message and exit</p>
</dd>
<dt><kbd><span class="option">-p <var>PATH</var></span>, <span class="option">--prefix <var>PATH</var></span></kbd></dt>
<dd><p>Path for log, dbUtil, and status files (default: “.”). If ends with
non-directory component, use as prefix for these files names (default:
TASKFILE_JOBID).</p>
</dd>
<dt><kbd><span class="option">--logfile <var>FILE</var></span></kbd></dt>
<dd><p>Log file.</p>
</dd>
<dt><kbd><span class="option">--mailFreq <var>N</var></span></kbd></dt>
<dd><p>Send email every N task completions (default: 1). “–mailTo” must be
given.</p>
</dd>
<dt><kbd><span class="option">--mailTo <var>ADDR</var></span></kbd></dt>
<dd><p>Mail address for task completion notification(s).</p>
</dd>
<dt><kbd><span class="option">-S</span>, <span class="option">--startup-only</span></kbd></dt>
<dd><p>Startup only the disBatch server (and KVS server if appropriate).
Use “dbUtil…” script to add execution contexts.
Incompatible with “–ssh-node”.</p>
</dd>
<dt><kbd><span class="option">-r <var>STATUSFILE</var></span>, <span class="option">--resume-from <var>STATUSFILE</var></span></kbd></dt>
<dd><p>Read the status file from a previous run and skip any completed tasks (may
be specified multiple times).</p>
</dd>
<dt><kbd><span class="option">-R</span>, <span class="option">--retry</span></kbd></dt>
<dd><p>With -r, also retry any tasks which failed in previous runs (non-zero
return).</p>
</dd>
<dt><kbd><span class="option">--force-resume</span></kbd></dt>
<dd><p>With -r, proceed even if task commands/lines are different.</p>
</dd>
<dt><kbd><span class="option">-e</span>, <span class="option">--exit-code</span></kbd></dt>
<dd><p>When any task fails, exit with non-zero status (default: only if disBatch
itself fails)</p>
</dd>
<dt><kbd><span class="option">-w</span>, <span class="option">--web</span></kbd></dt>
<dd><p>Enable web interface.</p>
</dd>
</dl>
<dl class="simple">
<dt>–kvsserver [HOST:PORT]</dt><dd><p>Use a running KVS server.</p>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">--taskcommand <var>COMMAND</var></span></kbd></dt>
<dd><p>Tasks will come from the command specified via the KVS server (passed in
the environment).</p>
</dd>
</dl>
<dl class="simple">
<dt>–taskserver [HOST:PORT]</dt><dd><p>Tasks will come from the KVS server.</p>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">-C <var>TASK_LIMIT</var></span>, <span class="option">--context-task-limit <var>TASK_LIMIT</var></span></kbd></dt>
<dd><p>Shutdown after running COUNT tasks (0 =&gt; no limit).</p>
</dd>
<dt><kbd><span class="option">-c <var>N</var></span>, <span class="option">--cpusPerTask <var>N</var></span></kbd></dt>
<dd><p>Number of cores used per task; may be fractional (default: 1).</p>
</dd>
<dt><kbd><span class="option">-g</span>, <span class="option">--gpu</span></kbd></dt>
<dd><p>Use assigned GPU resources</p>
</dd>
<dt><kbd><span class="option">-k <var>COMMAND</var></span>, <span class="option">--retire-cmd <var>COMMAND</var></span></kbd></dt>
<dd><p>Shell command to run to retire a node (environment includes $NODE being
retired, remaining $ACTIVE node list, $RETIRED node list; default based on
batch system). Incompatible with “–ssh-node”.</p>
</dd>
<dt><kbd><span class="option">-K</span>, <span class="option">--no-retire</span></kbd></dt>
<dd><p>Don’t retire nodes from the batch system (e.g., if running as part of a
larger job); equivalent to -k ‘’.</p>
</dd>
<dt><kbd><span class="option">-l <var>COMMAND</var></span>, <span class="option">--label <var>COMMAND</var></span></kbd></dt>
<dd><p>Label for this context. Should be unique.</p>
</dd>
</dl>
<dl class="simple">
<dt>-s HOST:COUNT, –ssh-node HOST:COUNT</dt><dd><p>Run tasks over SSH on the given nodes (can be specified multiple times for
additional hosts; equivalent to setting DISBATCH_SSH_NODELIST)</p>
</dd>
</dl>
<dl class="option-list">
<dt><kbd><span class="option">-t <var>N</var></span>, <span class="option">--tasksPerNode <var>N</var></span></kbd></dt>
<dd><p>Maximum concurrently executing tasks per node (up to cores/cpusPerTask).</p>
</dd>
</dl>
</dd>
</dl>
<hr class="docutils" />
<p>The options for mail will only work if your computing environment permits processes to access mail via SMTP.</p>
<p>A value for <cite>-c</cite> &lt; 1 effectively allows you to run more tasks concurrently than CPUs specified for the run. This is somewhat unusual, and generally not recommended, but could be appropriate in some cases.</p>
<p>The <cite>-k</cite> and <cite>-K</cite> flags allow you to control what disBatch does when a node is no longer needed to run jobs.
When running under slurm, disBatch will by default run the command:</p>
<blockquote>
<div><p>scontrol update JobId=”$SLURM_JOBID” NodeList=”${DRIVER_NODE:+$DRIVER_NODE,}$ACTIVE”</p>
</div></blockquote>
<p>which will tell slurm to release any nodes no longer being used.
You can set this to run a different command, or nothing at all.
While running this command, the follow environment variables will be set: <cite>NODE</cite> (the node that is no longer needed), <cite>ACTIVE</cite> (a comma-delimited list of nodes that are still active), <cite>RETIRED</cite> (a comma-delimited list of nodes that are no longer active, including <cite>$NODE</cite>), and possibly <cite>DRIVER_NODE</cite> (the node still running the main disBatch script, if it’s not in <cite>ACTIVE</cite>).</p>
<p>The <cite>-g</cite> argument parses the CUDA environment varables (<cite>CUDA_VISIBLE_DEVICES</cite>, <cite>GPU_DEVICE_ORDINAL</cite>) provided on each node and divides the resources between the running tasks.  For example, with slurm, if you want to run on _n_ nodes, with _t_ tasks per node, each using _c_ CPUs and 1 GPU (that is, _tc_ CPUs and _t_ GPUs per node, or _ntc_ CPUs and _nt_ GPUs total), you can do:</p>
<blockquote>
<div><p>sbatch -N$n -c$c –ntasks-per-node=$t –gres=gpu:$t -p gpu –wrap ‘disBatch.py -g $taskfile’`</p>
</div></blockquote>
<p><cite>-S</cite> Starts disBatch in a mode in which it waits for execution resources to be added. In this mode, disBatch starts up the task management system and
generates a script <cite>&lt;Prefix&gt;_dbUtil.sh</cite>, where <cite>&lt;Prefix&gt;</cite> refers to the <cite>-p</cite> option or default, see above. We’ll call this simply <cite>dbUtils.sh</cite> here,
but remember to include <cite>&lt;Prefix&gt;_</cite> in actual use. You can add execution resources by doing one or more of the following multiple times:
1. Submit <cite>dbUtils.sh</cite> as a job, e.g.:</p>
<blockquote>
<div><p><cite>sbatch -N 5 –ntasks-per-node 7 dbUtil.sh</cite></p>
</div></blockquote>
<ol class="arabic" start="2">
<li><p>Use ssh, e.g.:</p>
<blockquote>
<div><p><cite>./dbUtil.sh -s localhost:4,friendlyNeighbor:5</cite></p>
</div></blockquote>
</li>
</ol>
<p>Each of these creates an execution <em>context</em>, which contains one of more execution <em>engines</em> (five engines in the first, two in the second).
An engine can run one or more tasks currently. In the first example, each of the 5 engines will run up to 7 tasks concurrently, while in the
second example, the engine on <cite>localhost</cite> will run up to 4 tasks concurrently and the engine on <cite>friendlyNeighbor</cite> will run up to 5.
<cite>./dbUtil.sh –mon</cite> will start a simple ASCII-based monitor that tracks the overall state of the disBatch run, and the activity of the individual
contexts and engines. By cursoring over an engine, you can send a shutdown signal to the engine or its context. This signal is <em>soft</em>, triggering
a graceful shutdown that will occur only after currently assigned tasks are complete. Other execution resources are uneffected.</p>
<p>When a context is started, you can also supply the argument <cite>–context-task-limit N</cite>. This will shutdown the context and all associated engines
after it has run <cite>N</cite> tasks.</p>
<p>Taken together, these mechanisms enable disBatch to run on a dynamic pool of execution resources, so you can “borrow” a colleague’s workstation overnight, or
claim a large chunk of a currently idle partition, but return some if demands picks up, or chain together a series of time limited allocations to
accomplish a long run. When using this mode, keep in mind two caveats: (i) The time quantum is determined by your task duration. If any given task might
run for hours or days, then the utility of this is limited. You can still use standard means (kill, scancel) to terminate contexts and engines, but
you will likely have incomplete tasks to
reckon with; (ii) The task manangement system must itself be run in a setting where a long lived process is OK. Say in a <cite>screen</cite> or <cite>tmux</cite> session on
the login node of a cluster, or on your personal workstation (assuming it has the appropriate connectivity to reach the other resources you plan to use).</p>
<p><cite>-r</cite> uses the status file of a previous run to determine what tasks to run during this disBatch invocation. Only those tasks that haven’t yet run (or with <cite>-R</cite>, those that haven’t run or did but returned a non-0 exit code) are run this time. By default, the numeric task identifier and the text of the command are used to determine if a current task is the same as one found in the status file. <cite>–force-resume</cite> restricts the comparison to just the numeric identifier.</p>
<p><cite>–kvsserver</cite>, <cite>–taskcommand</cite>, and <cite>–taskserver</cite> implement advanced functionality (placing disBatch in an existing shared key store context and allowing for a programmatic rather than textual task interface). Contact the authors for more details.</p>
<p>### Considerations for large runs</p>
<p>If you do submit jobs with order 10000 or more tasks, you should
carefully consider how you want to organize the output (and error) files
produced by each of the tasks. It is generally a bad idea to have more
than a few thousand files in any one directory, so you will probably
want to introduce at least one extra level of directory hierarchy so
that the files can be divided into smaller groups. Intermediate
directory <cite>13</cite>, say, might hold all the files for tasks 13000 to
13999.</p>
<p>## #DISBATCH directives</p>
<p>### PREFIX and SUFFIX</p>
<p>In order to simplify task files, disBatch supports a couple of
directives to specify common task prefix strings and suffix strings. It
also provides environment variables to identify various aspects of the
submission. Here’s an example</p>
<blockquote>
<div><p># Note there is a space at the end of the next line.
#DISBATCH PREFIX ( cd /path/to/workdir ; source SetupEnv ;
#DISBATCH SUFFIX  ) &amp;&gt; ${DISBATCH_NAMETASKS}_${DISBATCH_JOBID}_${DISBATCH_TASKID}.log</p>
</div></blockquote>
<p>These are textually prepended and appended, respectively, to the text of
each subsequent task line. If the suffix includes redirection and a task is a proper command sequence (a series of
commands joined by <cite>;</cite>), then the task should be wrapped in <cite>( … )</cite>, as in this example, so that the standard error and standard output of the whole sequence
will be redirected to the log file. If this is not done, only standard
error and standard output for the last component of the command sequence
will be captured. This is probably not what you want unless you have
redirected these outputs for the previous individual parts of the
command sequence.</p>
<p>Using these, the above commands could be replaced with:</p>
<blockquote>
<div><p>myprog -a 0 -b 0 -c 0
myprog -a 0 -b 0 -c 1
…
myprog -a 9 -b 9 -c 8
myprog -a 9 -b 9 -c 9</p>
</div></blockquote>
<p>Note: the log files will have a different naming scheme, but there will still be one per task.</p>
<p>Later occurrences of <cite>#DISBATCH PREFIX</cite> or <cite>#DISBATCH SUFFIX</cite> in a task
file simply replace previous ones. When these are used, the tasks
reported in the status file include the prefix and suffix in
force at the time the task was launched.</p>
<p>### BARRIER</p>
<p>If your tasks fall into groups where a later group should only begin
after all tasks of the previous group have completely finished, you can
use this directive:</p>
<blockquote>
<div><p>#DISBATCH BARRIER</p>
</div></blockquote>
<p>When disBatch encounters this directive, it will not launch another task
until all tasks in progress have completed. Advanced feature: if <cite>BARRIER</cite> is followed by a string, that string is interpreted as a key name. When the barrier completes, the Task ID of the barrier will be <cite>put</cite> to that key.</p>
<p>### REPEAT</p>
<p>For those problems that are easily handled via a job-array-like approach:</p>
<blockquote>
<div><p>#DISBATCH REPEAT 5 start 100 step 50 [command]</p>
</div></blockquote>
<p>will expand into five tasks, each with the environment variable
<cite>DISBATCH_REPEAT_INDEX</cite> set to one of 100, 150, 200, 250, or 300.
The tasks will consist of the concatenation of the prefix, command (if provided),
and the suffix currently in effect. <cite>start</cite> defaults to 0, <cite>step</cite>
to 1. Note: the semantics here differ somewhat from many range
constructs, the number immediately following <cite>REPEAT</cite> sets the
number of tasks that will be executed; the next two numbers affect
only the value that the repeat index will have in
the environment for each of the repeat task instances. So, returning to our earlier example, the task file
could be:</p>
<blockquote>
<div><p>#DISBATCH PREFIX  ( cd /path/to/workdir ; a=$((DISBATCH_REPEAT_INDEX/100)) b=$(((DISBATCH_REPEAT_INDEX%100)/10 )) c=$((DISBATCH_REPEAT_INDEX%10)) ; myprog -a $a -b $b -c $c ) &amp;&gt; task_${a}_${b}_${c}.log
#DISBATCH REPEAT 1000</p>
</div></blockquote>
<p>This is not a model of clarify, but does illustrate that the repeat constuct can be relatively powerful. Many users may find it more convenient to use the tool of their choice to generate a text file with 1000 invocations explictly written out.</p>
<p>## License</p>
<p>Copyright 2017 Simons Foundation</p>
<p>Licensed under the Apache License, Version 2.0 (the “License”);
you may not use this file except in compliance with the License.
You may obtain a copy of the License at</p>
<blockquote>
<div><p><a class="reference external" href="http://www.apache.org/licenses/LICENSE-2.0">http://www.apache.org/licenses/LICENSE-2.0</a></p>
</div></blockquote>
<p>Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an “AS IS” BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.</p>
</section>
</section>
<section id="contents">
<h1>Contents:<a class="headerlink" href="#contents" title="Permalink to this headline"></a></h1>
<div class="toctree-wrapper compound">
</div>
</section>
<section id="feedback">
<h1>Feedback<a class="headerlink" href="#feedback" title="Permalink to this headline"></a></h1>
<p>If you encounter any errors or problems with <strong>disBatch</strong>, please open a GitHub issue at
<a class="reference external" href="https://github.com/flatironinstitute/disBatch">https://github.com/flatironinstitute/disBatch</a>.</p>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2020, Jerry Vinokurov.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>